{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811292fd",
   "metadata": {},
   "source": [
    "somesh code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96945e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Integrated keyword identification + YAKE + SQLite + visual comparison script.\n",
    "\n",
    "Features:\n",
    "- Scrape and clean text from URLs\n",
    "- Count word frequencies (single words)\n",
    "- Extract keywords/phrases using YAKE (multi-word allowed)\n",
    "- Store results in SQLite database (persistent)\n",
    "- Visualize using bar charts + word clouds\n",
    "- Compare top-N keywords between two URLs (Option 1 style comparison)\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# IMPORTS\n",
    "# -----------------------------------------\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "import yake\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -----------------------------------------\n",
    "# SCRAPING + CLEANING\n",
    "# -----------------------------------------\n",
    "def scrape_and_clean(url: str, timeout: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the web page and return cleaned text (letters only, lowercase).\n",
    "    - Uses requests to GET the page\n",
    "    - Uses BeautifulSoup to extract visible text\n",
    "    - Cleans with regex: keep letters and spaces, lowercases\n",
    "    Returns empty string on failure.\n",
    "    \"\"\"\n",
    "    print(f\"[SCRAPE] Fetching: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to fetch {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Get textual content (a simple method: the page's text)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # get_text(separator=\" \") helps keep words separated when tags are removed\n",
    "    raw_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    # Clean: remove non-letter characters, collapse multiple spaces, lowercase\n",
    "    cleaned = re.sub(r\"[^A-Za-z\\s]\", \" \", raw_text)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip().lower()\n",
    "    return cleaned\n",
    "\n",
    "# -----------------------------------------\n",
    "# WORD FREQUENCY (SINGLE WORDS)\n",
    "# -----------------------------------------\n",
    "def count_word_frequencies(clean_text: str) -> Counter:\n",
    "    \"\"\"\n",
    "    Count single-word frequencies from cleaned text.\n",
    "    Returns a collections.Counter mapping word -> frequency.\n",
    "    \"\"\"\n",
    "    if not clean_text:\n",
    "        return Counter()\n",
    "    words = clean_text.split()\n",
    "    # Optionally remove short words (single-letter except 'a' and 'i') if needed:\n",
    "    words = [w for w in words if not (len(w) == 1 and w not in {'a', 'i'})]\n",
    "    return Counter(words)\n",
    "\n",
    "# -----------------------------------------\n",
    "# YAKE EXTRACTION (multi-word allowed)\n",
    "# -----------------------------------------\n",
    "def extract_keywords_yake(clean_text: str, top_n: int = 20, max_ngram_size: int = 3) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Use YAKE to extract keywords/phrases.\n",
    "    - clean_text: the cleaned page text\n",
    "    - top_n: how many keywords to return\n",
    "    - max_ngram_size: 1 for single words, 2-3 to allow phrases\n",
    "    Returns a list of (keyword, score) where lower score => more relevant.\n",
    "    \"\"\"\n",
    "    if not clean_text:\n",
    "        return []\n",
    "\n",
    "    # Initialize YAKE extractor with English defaults and chosen n-gram size\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan=\"en\",\n",
    "        n=max_ngram_size,\n",
    "        dedupLim=0.9,   # deduplication threshold (0..1)\n",
    "        top=top_n,\n",
    "        features=None\n",
    "    )\n",
    "    keywords = kw_extractor.extract_keywords(clean_text)\n",
    "    # YAKE returns (keyword, score) tuples\n",
    "    return keywords\n",
    "\n",
    "# -----------------------------------------\n",
    "# DATABASE (SQLite) - CREATE / SAVE / READ\n",
    "# -----------------------------------------\n",
    "def init_db(db_path: str = \"keywords_compare.db\") -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    Create/connect the SQLite DB and ensure required tables exist.\n",
    "    Tables:\n",
    "      - word_counts(url, word, frequency)         : single-word frequencies\n",
    "      - yake_keywords(url, keyword, score)        : YAKE keywords & scores\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create tables; simple schema (no ID column required for this app)\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS word_counts (\n",
    "        url TEXT NOT NULL,\n",
    "        word TEXT NOT NULL,\n",
    "        frequency INTEGER NOT NULL,\n",
    "        UNIQUE(url, word)\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS yake_keywords (\n",
    "        url TEXT NOT NULL,\n",
    "        keyword TEXT NOT NULL,\n",
    "        score REAL NOT NULL,\n",
    "        UNIQUE(url, keyword)\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Commit and return connection\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def save_word_counts(conn: sqlite3.Connection, url: str, counter: Counter):\n",
    "    \"\"\"\n",
    "    Save (url, word, frequency) into word_counts table.\n",
    "    We replace existing rows for the url (simple approach: delete then insert).\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DELETE FROM word_counts WHERE url = ?\", (url,))\n",
    "    data = [(url, w, int(freq)) for w, freq in counter.items()]\n",
    "    # Use executemany for efficiency\n",
    "    cur.executemany(\"INSERT OR REPLACE INTO word_counts (url, word, frequency) VALUES (?, ?, ?)\", data)\n",
    "    conn.commit()\n",
    "\n",
    "def save_yake_keywords(conn: sqlite3.Connection, url: str, keywords: List[Tuple[str, float]]):\n",
    "    \"\"\"\n",
    "    Save YAKE keywords into yake_keywords table.\n",
    "    We delete existing rows for url and insert the new list.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DELETE FROM yake_keywords WHERE url = ?\", (url,))\n",
    "    data = [(url, kw, float(score)) for kw, score in keywords]\n",
    "    cur.executemany(\"INSERT OR REPLACE INTO yake_keywords (url, keyword, score) VALUES (?, ?, ?)\", data)\n",
    "    conn.commit()\n",
    "\n",
    "def get_top_n_words(conn: sqlite3.Connection, url: str, n: int = 10) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Return top-n words (word, frequency) ordered by frequency desc for a url.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT word, frequency FROM word_counts\n",
    "        WHERE url = ?\n",
    "        ORDER BY frequency DESC\n",
    "        LIMIT ?\n",
    "    \"\"\", (url, n))\n",
    "    return cur.fetchall()\n",
    "\n",
    "def get_top_n_yake(conn: sqlite3.Connection, url: str, n: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Return top-n YAKE keywords (keyword, score) ordered by ascending score (lower = better).\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT keyword, score FROM yake_keywords\n",
    "        WHERE url = ?\n",
    "        ORDER BY score ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (url, n))\n",
    "    return cur.fetchall()\n",
    "\n",
    "# -----------------------------------------\n",
    "# VISUALIZATION HELPERS: BAR CHARTS + WORDCLOUDS\n",
    "# -----------------------------------------\n",
    "def plot_bar_grouped_freqs(conn: sqlite3.Connection, url1: str, url2: str, top_n: int = 10, figsize=(10,5)):\n",
    "    \"\"\"\n",
    "    Compare the top-N frequency words from url1 and url2 using a grouped bar chart.\n",
    "    We use the union of the two top-list words to show side-by-side counts (0 if absent).\n",
    "    \"\"\"\n",
    "    top1 = get_top_n_words(conn, url1, top_n)\n",
    "    top2 = get_top_n_words(conn, url2, top_n)\n",
    "\n",
    "    # Words sets & union\n",
    "    words1 = [w for w, f in top1]\n",
    "    words2 = [w for w, f in top2]\n",
    "    union_words = list(dict.fromkeys(words1 + words2))  # maintain order: url1 then url2 extras\n",
    "\n",
    "    # Build arrays of frequencies aligned to union_words\n",
    "    freqs1 = []\n",
    "    freqs2 = []\n",
    "    freq_map1 = {w: f for w, f in top1}\n",
    "    freq_map2 = {w: f for w, f in top2}\n",
    "    for w in union_words:\n",
    "        freqs1.append(freq_map1.get(w, 0))\n",
    "        freqs2.append(freq_map2.get(w, 0))\n",
    "\n",
    "    x = np.arange(len(union_words))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.bar(x - width/2, freqs1, width, label=url1)\n",
    "    plt.bar(x + width/2, freqs2, width, label=url2)\n",
    "    plt.xticks(x, union_words, rotation=45, ha='right')\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Top-{top_n} Frequency Keyword Comparison\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_bar_grouped_yake(conn: sqlite3.Connection, url1: str, url2: str, top_n: int = 10, figsize=(10,5)):\n",
    "    \"\"\"\n",
    "    Compare top-N YAKE keywords for both URLs.\n",
    "    YAKE scores: lower = better; we invert scores to show 'importance' (higher = more important)\n",
    "    \"\"\"\n",
    "    y1 = get_top_n_yake(conn, url1, top_n)\n",
    "    y2 = get_top_n_yake(conn, url2, top_n)\n",
    "\n",
    "    kws1 = [kw for kw, s in y1]\n",
    "    kws2 = [kw for kw, s in y2]\n",
    "    union_kws = list(dict.fromkeys(kws1 + kws2))\n",
    "\n",
    "    imp1 = []\n",
    "    imp2 = []\n",
    "    map1 = {kw: (1.0/score if score>0 else 0.0) for kw, score in y1}\n",
    "    map2 = {kw: (1.0/score if score>0 else 0.0) for kw, score in y2}\n",
    "\n",
    "    for kw in union_kws:\n",
    "        imp1.append(map1.get(kw, 0.0))\n",
    "        imp2.append(map2.get(kw, 0.0))\n",
    "\n",
    "    x = np.arange(len(union_kws))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.bar(x - width/2, imp1, width, label=url1)\n",
    "    plt.bar(x + width/2, imp2, width, label=url2)\n",
    "    plt.xticks(x, union_kws, rotation=45, ha='right')\n",
    "    plt.ylabel(\"Inverted YAKE score (higher = more relevant)\")\n",
    "    plt.title(f\"Top-{top_n} YAKE Keyword Comparison (inverted scores shown)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_wordcloud_from_freqs(freq_map: dict, title: str = \"WordCloud\", figsize=(10,5)):\n",
    "    \"\"\"\n",
    "    Generic wordcloud generator from a {word:weight} mapping.\n",
    "    \"\"\"\n",
    "    if not freq_map:\n",
    "        print(\"[WORDCLOUD] No data to plot.\")\n",
    "        return\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(freq_map)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_wordcloud_frequencies(conn: sqlite3.Connection, url: str, top_n: int = 50):\n",
    "    \"\"\"\n",
    "    Create wordcloud from top-N frequency words (weights = frequency).\n",
    "    \"\"\"\n",
    "    top = get_top_n_words(conn, url, top_n)\n",
    "    freq_map = {w: f for w, f in top}\n",
    "    plot_wordcloud_from_freqs(freq_map, title=f\"Frequency WordCloud for {url}\")\n",
    "\n",
    "def plot_wordcloud_yake(conn: sqlite3.Connection, url: str, top_n: int = 50):\n",
    "    \"\"\"\n",
    "    Create wordcloud from top-N YAKE keywords.\n",
    "    Convert YAKE score -> importance by inversion (1/score).\n",
    "    \"\"\"\n",
    "    top = get_top_n_yake(conn, url, top_n)\n",
    "    # Avoid division by zero\n",
    "    wmap = {}\n",
    "    for kw, score in top:\n",
    "        try:\n",
    "            wmap[kw] = 1.0 / float(score) if float(score) > 0 else 0.0\n",
    "        except Exception:\n",
    "            wmap[kw] = 0.0\n",
    "    plot_wordcloud_from_freqs(wmap, title=f\"YAKE WordCloud for {url}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# COMPARISON — Option 1: Top-N lists comparison\n",
    "# -----------------------------------------\n",
    "def compare_top_n_between_urls(conn: sqlite3.Connection, url1: str, url2: str, top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Compare top-N words and YAKE keywords between url1 and url2.\n",
    "    Steps:\n",
    "      - Fetch top-N by frequency and by YAKE for each URL\n",
    "      - Compute common and unique keywords (for each method)\n",
    "      - Print summary (counts and lists)\n",
    "      - Plot grouped bar charts for frequency & YAKE (inverted scores)\n",
    "      - Show wordclouds side by side (caller can call individually)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== COMPARISON SUMMARY ===\")\n",
    "    # Frequency-based comparison:\n",
    "    top_freq_1 = get_top_n_words(conn, url1, top_n)\n",
    "    top_freq_2 = get_top_n_words(conn, url2, top_n)\n",
    "    set_freq1 = set([w for w, _ in top_freq_1])\n",
    "    set_freq2 = set([w for w, _ in top_freq_2])\n",
    "\n",
    "    common_freq = set_freq1.intersection(set_freq2)\n",
    "    unique_1_freq = set_freq1 - set_freq2\n",
    "    unique_2_freq = set_freq2 - set_freq1\n",
    "\n",
    "    print(f\"\\nFrequency-based top-{top_n} comparison:\")\n",
    "    print(f\" - {url1} top words: {list(set_freq1)}\")\n",
    "    print(f\" - {url2} top words: {list(set_freq2)}\")\n",
    "    print(f\" - Common words ({len(common_freq)}): {sorted(list(common_freq))}\")\n",
    "    print(f\" - Unique to {url1} ({len(unique_1_freq)}): {sorted(list(unique_1_freq))}\")\n",
    "    print(f\" - Unique to {url2} ({len(unique_2_freq)}): {sorted(list(unique_2_freq))}\")\n",
    "\n",
    "    # YAKE-based comparison:\n",
    "    top_yake_1 = get_top_n_yake(conn, url1, top_n)\n",
    "    top_yake_2 = get_top_n_yake(conn, url2, top_n)\n",
    "    set_yake1 = set([kw for kw, _ in top_yake_1])\n",
    "    set_yake2 = set([kw for kw, _ in top_yake_2])\n",
    "\n",
    "    common_yake = set_yake1.intersection(set_yake2)\n",
    "    unique_1_yake = set_yake1 - set_yake2\n",
    "    unique_2_yake = set_yake2 - set_yake1\n",
    "\n",
    "    print(f\"\\nYAKE-based top-{top_n} comparison:\")\n",
    "    print(f\" - {url1} YAKE top: {list(set_yake1)}\")\n",
    "    print(f\" - {url2} YAKE top: {list(set_yake2)}\")\n",
    "    print(f\" - Common YAKE keywords ({len(common_yake)}): {sorted(list(common_yake))}\")\n",
    "    print(f\" - Unique to {url1} ({len(unique_1_yake)}): {sorted(list(unique_1_yake))}\")\n",
    "    print(f\" - Unique to {url2} ({len(unique_2_yake)}): {sorted(list(unique_2_yake))}\")\n",
    "\n",
    "    # Visual comparisons via grouped bar charts:\n",
    "    print(\"\\n[Visual] Generating grouped bar charts for frequency and YAKE comparisons...\")\n",
    "    plot_bar_grouped_freqs(conn, url1, url2, top_n)\n",
    "    plot_bar_grouped_yake(conn, url1, url2, top_n)\n",
    "\n",
    "    # Wordclouds (separate)\n",
    "    print(\"\\n[Visual] Generating word clouds (frequency & YAKE) for both URLs...\")\n",
    "    plot_wordcloud_frequencies(conn, url1, top_n*2)  # show a bigger cloud\n",
    "    plot_wordcloud_yake(conn, url1, top_n*2)\n",
    "    plot_wordcloud_frequencies(conn, url2, top_n*2)\n",
    "    plot_wordcloud_yake(conn, url2, top_n*2)\n",
    "\n",
    "# -----------------------------------------\n",
    "# MAIN: Run the pipeline for two URLs and compare (Option 1)\n",
    "# -----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URLs; replace with any two you want to compare\n",
    "    url_a = \"https://www.iisermohali.ac.in\"\n",
    "    url_b = \"https://www.iiserpune.ac.in\"\n",
    "\n",
    "    # Initialize DB\n",
    "    conn = init_db(\"keywords_compare.db\")\n",
    "\n",
    "    # Process both URLs\n",
    "    for url in (url_a, url_b):\n",
    "        cleaned = scrape_and_clean(url)\n",
    "        if not cleaned:\n",
    "            continue  # skip if fetch failed\n",
    "\n",
    "        # Word frequencies\n",
    "        counter = count_word_frequencies(cleaned)\n",
    "        save_word_counts(conn, url, counter)\n",
    "\n",
    "        # YAKE extraction\n",
    "        yake_list = extract_keywords_yake(cleaned, top_n=20, max_ngram_size=3)\n",
    "        # yake_list is [(keyword, score), ...] with lower score better\n",
    "        save_yake_keywords(conn, url, yake_list)\n",
    "\n",
    "        # Print quick top results\n",
    "        print(f\"\\nTop 10 frequency words for {url}:\")\n",
    "        for w, f in get_top_n_words(conn, url, 10):\n",
    "            print(f\"  {w:>15} : {f}\")\n",
    "\n",
    "        print(f\"\\nTop 10 YAKE keywords for {url}:\")\n",
    "        for kw, score in get_top_n_yake(conn, url, 10):\n",
    "            print(f\"  {kw:>25} : {score:.6f}\")\n",
    "\n",
    "    # Run Option 1 comparison (top-10)\n",
    "    compare_top_n_between_urls(conn, url_a, url_b, top_n=10)\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc374b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.iisermohali.ac.in/\n",
      "Unique words (Mohali): 212\n",
      "Top 10 (Mohali): [('iiser', 15), ('mohali', 14), ('of', 8), ('science', 6), ('sciences', 6), ('committee', 6), ('research', 5), ('day', 5), ('students', 5), ('institute', 4)]\n",
      "Stored Mohali counts in DB.\n",
      "\n",
      "Scraping: https://www.iiserpune.ac.in/\n",
      "Stored Pune counts in DB.\n",
      "\n",
      "Top words - Mohali:\n",
      "          iiser  30\n",
      "         mohali  28\n",
      "             of  16\n",
      "      committee  12\n",
      "        science  12\n",
      "       sciences  12\n",
      "            day  10\n",
      "       research  10\n",
      "       students  10\n",
      "             at  8\n",
      "        faculty  8\n",
      "      institute  8\n",
      "           news  8\n",
      "      academics  6\n",
      "            ble  6\n",
      "             by  6\n",
      "       calendar  6\n",
      "    celebration  6\n",
      "     committees  6\n",
      "         events  6\n",
      "\n",
      "Top words - Pune:\n",
      "            and  72\n",
      "            the  44\n",
      "             in  40\n",
      "             of  34\n",
      "          iiser  30\n",
      "           pune  30\n",
      "       research  30\n",
      "         events  22\n",
      "             on  22\n",
      "         campus  20\n",
      "        faculty  20\n",
      "             at  18\n",
      "            for  18\n",
      "        science  18\n",
      "             by  16\n",
      "        fellows  16\n",
      "        members  16\n",
      "           more  16\n",
      "           news  16\n",
      "         posted  16\n",
      "\n",
      "Top common words (by sum of frequencies):\n",
      "          iiser  Mohali:   30  Pune:   30\n",
      "             of  Mohali:   16  Pune:   34\n",
      "            the  Mohali:    6  Pune:   44\n",
      "             in  Mohali:    4  Pune:   40\n",
      "       research  Mohali:   10  Pune:   30\n",
      "        science  Mohali:   12  Pune:   18\n",
      "         events  Mohali:    6  Pune:   22\n",
      "        faculty  Mohali:    8  Pune:   20\n",
      "             at  Mohali:    8  Pune:   18\n",
      "         campus  Mohali:    4  Pune:   20\n",
      "           news  Mohali:    8  Pune:   16\n",
      "             by  Mohali:    6  Pune:   16\n",
      "            for  Mohali:    4  Pune:   18\n",
      "       sciences  Mohali:   12  Pune:    8\n",
      "             dr  Mohali:    4  Pune:   14\n",
      "        members  Mohali:    2  Pune:   16\n",
      "             to  Mohali:    4  Pune:   14\n",
      "         alumni  Mohali:    2  Pune:   14\n",
      "            day  Mohali:   10  Pune:    6\n",
      "             as  Mohali:    2  Pune:   12\n",
      "      committee  Mohali:   12  Pune:    2\n",
      "        elected  Mohali:    2  Pune:   12\n",
      "     facilities  Mohali:    6  Pune:    8\n",
      "          india  Mohali:    4  Pune:   10\n",
      "          media  Mohali:    2  Pune:   12\n",
      "\n",
      "Top words unique to Mohali (not in Pune):\n",
      "         mohali  28\n",
      "    celebration  6\n",
      "     committees  6\n",
      "      president  6\n",
      "           vice  6\n",
      "   achievements  4\n",
      "        awarded  4\n",
      "        careers  4\n",
      "     complaints  4\n",
      "       computer  4\n",
      "       festival  4\n",
      "        holiday  4\n",
      "            icc  4\n",
      "     institutes  4\n",
      "          rules  4\n",
      "         school  4\n",
      "         system  4\n",
      "             th  4\n",
      "        website  4\n",
      "      academies  2\n",
      "         across  2\n",
      " administration  2\n",
      " administrative  2\n",
      "      admission  2\n",
      " advertisements  2\n",
      "       advisory  2\n",
      "       agencies  2\n",
      "           anil  2\n",
      "          anjan  2\n",
      "         annual  2\n"
     ]
    }
   ],
   "source": [
    "# Complete end-to-end: scrape -> clean -> count -> store -> compare\n",
    "# Requirements: requests, beautifulsoup4\n",
    "# Run: pip install requests beautifulsoup4  (if not installed)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -------------------------\n",
    "# Scrape + clean functions\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove non-letters except spaces,\n",
    "    remove stray single letters except 'a' and 'i', normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)               # keep only a-z and whitespace\n",
    "    text = re.sub(r'\\b(?![ai]\\b)[a-z]\\b', ' ', text)    # remove single letters except 'a' and 'i'\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()            # collapse spaces\n",
    "    return text\n",
    "\n",
    "def scrape_text_from_url(url: str, valid_tags=None, timeout=12) -> str:\n",
    "    \"\"\"\n",
    "    Fetch URL, parse HTML, extract text from valid_tags, then clean it.\n",
    "    Returns cleaned text string.\n",
    "    \"\"\"\n",
    "    if valid_tags is None:\n",
    "        valid_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']\n",
    "\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch {url} (status {response.status_code})\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pieces = [el.get_text(separator=' ', strip=True) for el in soup.find_all(valid_tags)]\n",
    "    raw_text = \" \".join(pieces)\n",
    "    return clean_text(raw_text)\n",
    "\n",
    "# -------------------------\n",
    "# Counting function\n",
    "# -------------------------\n",
    "def count_words(cleaned_text: str) -> Counter:\n",
    "    words = cleaned_text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "# -------------------------\n",
    "# SQLite storage functions (single table for all sites)\n",
    "# -------------------------\n",
    "def store_site_word_counts(\n",
    "    word_counts: Counter,\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    accumulate: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Store counts for a given site into SQLite.\n",
    "    - accumulate=True : add new counts to existing (useful for incremental scraping)\n",
    "    - accumulate=False: replace stored counts for that site (snapshot)\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table holding rows for many sites (site, word, frequency)\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            site TEXT NOT NULL,\n",
    "            word TEXT NOT NULL,\n",
    "            frequency INTEGER NOT NULL,\n",
    "            UNIQUE(site, word)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    if accumulate:\n",
    "        # add new counts to existing frequency\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = {table}.frequency + excluded.frequency\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # replace stored frequency with the new one\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = excluded.frequency\n",
    "        \"\"\"\n",
    "\n",
    "    for w, freq in word_counts.items():\n",
    "        cur.execute(upsert_sql, (site, w, int(freq)))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def read_top_by_site(\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 20\n",
    ") -> List[Tuple[str, int]]:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT word, frequency FROM {table} WHERE site = ? ORDER BY frequency DESC LIMIT ?\", (site, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "def compare_sites_common(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 30,\n",
    "    sort_by: str = \"sum\"  # \"sum\" | \"min\" | \"diff\"\n",
    ") -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Return common words between site1 and site2 as (word, freq_site1, freq_site2).\n",
    "    sort_by:\n",
    "      - 'sum' : sort by freq1+freq2 descending (default)\n",
    "      - 'min' : sort by min(freq1,freq2) descending -> words both strongly used\n",
    "      - 'diff': sort by absolute difference descending -> most differently-used words\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency AS f1, b.frequency AS f2\n",
    "        FROM {table} a\n",
    "        JOIN {table} b ON a.word = b.word\n",
    "        WHERE a.site = ? AND b.site = ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site1, site2))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if sort_by == \"sum\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: (r[1] + r[2]), reverse=True)\n",
    "    elif sort_by == \"min\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: min(r[1], r[2]), reverse=True)\n",
    "    elif sort_by == \"diff\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: abs(r[1] - r[2]), reverse=True)\n",
    "    else:\n",
    "        rows_sorted = rows\n",
    "\n",
    "    return rows_sorted[:top]\n",
    "\n",
    "def unique_to_site(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 50\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Words present in site1 but NOT in site2 (ordered by frequency in site1).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency\n",
    "        FROM {table} a\n",
    "        LEFT JOIN {table} b ON a.word = b.word AND b.site = ?\n",
    "        WHERE a.site = ? AND b.word IS NULL\n",
    "        ORDER BY a.frequency DESC\n",
    "        LIMIT ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site2, site1, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "# -------------------------\n",
    "# Example end-to-end usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: single site (IISER Mohali)\n",
    "    mohali_url = \"https://www.iisermohali.ac.in/\"\n",
    "    print(\"Scraping:\", mohali_url)\n",
    "    cleaned_mohali_text = scrape_text_from_url(mohali_url)\n",
    "    word_counts_mohali = count_words(cleaned_mohali_text)\n",
    "    print(\"Unique words (Mohali):\", len(word_counts_mohali))\n",
    "    print(\"Top 10 (Mohali):\", word_counts_mohali.most_common(10))\n",
    "\n",
    "    # Store in DB (accumulate=True => add counts if site already present)\n",
    "    store_site_word_counts(word_counts_mohali, site=mohali_url, accumulate=True)\n",
    "    print(\"Stored Mohali counts in DB.\")\n",
    "\n",
    "    # Example 2: another site (IISER Pune) - optional\n",
    "    pune_url = \"https://www.iiserpune.ac.in/\"\n",
    "    print(\"\\nScraping:\", pune_url)\n",
    "    cleaned_pune_text = scrape_text_from_url(pune_url)\n",
    "    word_counts_pune = count_words(cleaned_pune_text)\n",
    "    store_site_word_counts(word_counts_pune, site=pune_url, accumulate=True)\n",
    "    print(\"Stored Pune counts in DB.\")\n",
    "\n",
    "    # Read and display top words for each site\n",
    "    print(\"\\nTop words - Mohali:\")\n",
    "    for w, f in read_top_by_site(mohali_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "    print(\"\\nTop words - Pune:\")\n",
    "    for w, f in read_top_by_site(pune_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "\n",
    "    # Compare common words\n",
    "    print(\"\\nTop common words (by sum of frequencies):\")\n",
    "    for word, f1, f2 in compare_sites_common(mohali_url, pune_url, top=25, sort_by=\"sum\"):\n",
    "        print(f\"{word:>15}  Mohali:{f1:5d}  Pune:{f2:5d}\")\n",
    "\n",
    "    # Words unique to Mohali (not in Pune)\n",
    "    print(\"\\nTop words unique to Mohali (not in Pune):\")\n",
    "    for w, f in unique_to_site(mohali_url, pune_url, top=30):\n",
    "        print(f\"{w:>15}  {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68726e56",
   "metadata": {},
   "source": [
    "mine(with chat gpt help ofcourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bec134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing https://www.iisermohali.ac.in/ ---\n",
      "Scraped 344 words from https://www.iisermohali.ac.in/\n",
      "Top 10 Keywords:\n",
      "  iiser                           score=0.004085\n",
      "  mohali                          score=0.004981\n",
      "  day                             score=0.016648\n",
      "  research                        score=0.016648\n",
      "  committee                       score=0.017783\n",
      "  students                        score=0.018807\n",
      "  science                         score=0.019928\n",
      "  sciences                        score=0.019928\n",
      "  celebration                     score=0.020317\n",
      "  faculty                         score=0.021518\n",
      "  ble                             score=0.022610\n",
      "  president                       score=0.022610\n",
      "  vice                            score=0.022610\n",
      "  institute                       score=0.023509\n",
      "  hon                             score=0.024999\n",
      "  academics                       score=0.026343\n",
      "  calendar                        score=0.027477\n",
      "  facilities                      score=0.027477\n",
      "  events                          score=0.030039\n",
      "  policy                          score=0.030039\n",
      "\n",
      "--- Processing https://www.iiserpune.ac.in/ ---\n",
      "Scraped 730 words from https://www.iiserpune.ac.in/\n",
      "Top 10 Keywords:\n",
      "  iiser                           score=0.002319\n",
      "  pune                            score=0.002666\n",
      "  research                        score=0.003853\n",
      "  faculty                         score=0.004192\n",
      "  members                         score=0.004882\n",
      "  campus                          score=0.006308\n",
      "  events                          score=0.006472\n",
      "  posted                          score=0.006657\n",
      "  science                         score=0.006810\n",
      "  fellows                         score=0.006975\n",
      "  elected                         score=0.007267\n",
      "  read                            score=0.007848\n",
      "  alumni                          score=0.008624\n",
      "  media                           score=0.009034\n",
      "  national                        score=0.009977\n",
      "  prof                            score=0.010958\n",
      "  india                           score=0.012528\n",
      "  programme                       score=0.012745\n",
      "  sciences                        score=0.015323\n",
      "  book                            score=0.016534\n",
      "\n",
      "✅ YAKE keyword extraction completed and stored in database.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full Program: Web Scraper + Text Cleaner + YAKE Keyword Extraction + SQLite Storage\n",
    "-----------------------------------------------------------------------------------\n",
    "This program:\n",
    "1. Scrapes text from a given website (using requests + BeautifulSoup).\n",
    "2. Cleans the text (removes noise, normalizes case and spacing).\n",
    "3. Extracts important keywords/phrases using YAKE.\n",
    "4. Stores results into a SQLite database.\n",
    "5. Allows comparison between sites by keyword importance.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "import yake\n",
    "from typing import List, Tuple\n",
    "\n",
    "# =========================================================\n",
    "# STEP 1: Text cleaning\n",
    "# =========================================================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove non-alphabetic characters (except spaces),\n",
    "    and normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()                          # Normalize case\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)        # Keep only letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # Collapse multiple spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 2: Scraping text from a webpage\n",
    "# =========================================================\n",
    "def scrape_text_from_url(url: str, valid_tags=None, timeout: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Fetch webpage content, extract readable text, and clean it.\n",
    "    \"\"\"\n",
    "    if valid_tags is None:\n",
    "        valid_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'li']\n",
    "\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch {url} (status {response.status_code})\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract text only from specific HTML tags\n",
    "    pieces = [el.get_text(separator=' ', strip=True) for el in soup.find_all(valid_tags)]\n",
    "    raw_text = \" \".join(pieces)\n",
    "\n",
    "    # Clean and return\n",
    "    return clean_text(raw_text)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 3: YAKE Keyword Extraction\n",
    "# =========================================================\n",
    "def extract_keywords_yake(text: str, max_keywords: int = 20) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract top keywords using YAKE.\n",
    "    Lower score => more relevant keyword.\n",
    "    \"\"\"\n",
    "    # Configure YAKE parameters\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan=\"en\",              # language\n",
    "        n=1,                   # max n-gram size (1 = single words, 3 = up to trigrams)\n",
    "        dedupLim=0.9,          # threshold for merging similar words\n",
    "        top=max_keywords,      # number of keywords to extract\n",
    "        features=None          # use default YAKE features\n",
    "    )\n",
    "\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return keywords  # Returns list of (keyword, score)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 4: SQLite Storage\n",
    "# =========================================================\n",
    "def store_keywords(\n",
    "    keywords: List[Tuple[str, float]],\n",
    "    site: str,\n",
    "    db_path: str = \"keywords.db\",\n",
    "    table: str = \"keyword_scores\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Store YAKE keywords and scores for a site into SQLite database.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            site TEXT NOT NULL,\n",
    "            keyword TEXT NOT NULL,\n",
    "            score REAL NOT NULL,\n",
    "            UNIQUE(site, keyword)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert or update existing entries\n",
    "    for kw, score in keywords:\n",
    "        cur.execute(f\"\"\"\n",
    "            INSERT INTO {table} (site, keyword, score)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, keyword)\n",
    "            DO UPDATE SET score = excluded.score\n",
    "        \"\"\", (site, kw, score))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 5: Read keywords from DB\n",
    "# =========================================================\n",
    "def read_top_keywords(site: str, db_path: str = \"keywords.db\", table: str = \"keyword_scores\", top: int = 10):\n",
    "    \"\"\"\n",
    "    Fetch top N keywords (lowest YAKE score = most important).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT keyword, score FROM {table}\n",
    "        WHERE site = ?\n",
    "        ORDER BY score ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (site, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 6: Example run (compare two IISER sites)\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example sites\n",
    "    mohali_url = \"https://www.iisermohali.ac.in/\"\n",
    "    pune_url   = \"https://www.iiserpune.ac.in/\"\n",
    "\n",
    "    for site in [mohali_url, pune_url]:\n",
    "        print(f\"\\n--- Processing {site} ---\")\n",
    "\n",
    "        # Step 1: Scrape + Clean\n",
    "        text = scrape_text_from_url(site)\n",
    "        print(f\"Scraped {len(text.split())} words from {site}\")\n",
    "\n",
    "        # Step 2: Extract keywords using YAKE\n",
    "        keywords = extract_keywords_yake(text, max_keywords=20)\n",
    "\n",
    "        # Step 3: Store into SQLite\n",
    "        store_keywords(keywords, site)\n",
    "\n",
    "        # Step 4: Show top keywords\n",
    "        print(\"Top 10 Keywords:\")\n",
    "        for kw, score in read_top_keywords(site, top=30):\n",
    "            print(f\"  {kw:30}  score={score:.6f}\")\n",
    "\n",
    "    print(\"\\n✅ YAKE keyword extraction completed and stored in database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsenv2)",
   "language": "python",
   "name": "dsenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
