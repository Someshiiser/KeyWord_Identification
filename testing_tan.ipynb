{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811292fd",
   "metadata": {},
   "source": [
    "somesh code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc374b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Complete end-to-end: scrape -> clean -> count -> store -> compare\n",
    "# Requirements: requests, beautifulsoup4\n",
    "# Run: pip install requests beautifulsoup4  (if not installed)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -------------------------\n",
    "# Scrape + clean functions\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove non-letters except spaces,\n",
    "    remove stray single letters except 'a' and 'i', normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)               # keep only a-z and whitespace\n",
    "    text = re.sub(r'\\b(?![ai]\\b)[a-z]\\b', ' ', text)    # remove single letters except 'a' and 'i'\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()            # collapse spaces\n",
    "    return text\n",
    "\n",
    "def scrape_text_from_url(url: str, valid_tags=None, timeout=12) -> str:\n",
    "    \"\"\"\n",
    "    Fetch URL, parse HTML, extract text from valid_tags, then clean it.\n",
    "    Returns cleaned text string.\n",
    "    \"\"\"\n",
    "    if valid_tags is None:\n",
    "        valid_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']\n",
    "\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch {url} (status {response.status_code})\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pieces = [el.get_text(separator=' ', strip=True) for el in soup.find_all(valid_tags)]\n",
    "    raw_text = \" \".join(pieces)\n",
    "    return clean_text(raw_text)\n",
    "\n",
    "# -------------------------\n",
    "# Counting function\n",
    "# -------------------------\n",
    "def count_words(cleaned_text: str) -> Counter:\n",
    "    words = cleaned_text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "# -------------------------\n",
    "# SQLite storage functions (single table for all sites)\n",
    "# -------------------------\n",
    "def store_site_word_counts(\n",
    "    word_counts: Counter,\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    accumulate: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Store counts for a given site into SQLite.\n",
    "    - accumulate=True : add new counts to existing (useful for incremental scraping)\n",
    "    - accumulate=False: replace stored counts for that site (snapshot)\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table holding rows for many sites (site, word, frequency)\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            site TEXT NOT NULL,\n",
    "            word TEXT NOT NULL,\n",
    "            frequency INTEGER NOT NULL,\n",
    "            UNIQUE(site, word)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    if accumulate:\n",
    "        # add new counts to existing frequency\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = {table}.frequency + excluded.frequency\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # replace stored frequency with the new one\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = excluded.frequency\n",
    "        \"\"\"\n",
    "\n",
    "    for w, freq in word_counts.items():\n",
    "        cur.execute(upsert_sql, (site, w, int(freq)))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def read_top_by_site(\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 20\n",
    ") -> List[Tuple[str, int]]:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT word, frequency FROM {table} WHERE site = ? ORDER BY frequency DESC LIMIT ?\", (site, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "def compare_sites_common(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 30,\n",
    "    sort_by: str = \"sum\"  # \"sum\" | \"min\" | \"diff\"\n",
    ") -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Return common words between site1 and site2 as (word, freq_site1, freq_site2).\n",
    "    sort_by:\n",
    "      - 'sum' : sort by freq1+freq2 descending (default)\n",
    "      - 'min' : sort by min(freq1,freq2) descending -> words both strongly used\n",
    "      - 'diff': sort by absolute difference descending -> most differently-used words\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency AS f1, b.frequency AS f2\n",
    "        FROM {table} a\n",
    "        JOIN {table} b ON a.word = b.word\n",
    "        WHERE a.site = ? AND b.site = ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site1, site2))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if sort_by == \"sum\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: (r[1] + r[2]), reverse=True)\n",
    "    elif sort_by == \"min\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: min(r[1], r[2]), reverse=True)\n",
    "    elif sort_by == \"diff\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: abs(r[1] - r[2]), reverse=True)\n",
    "    else:\n",
    "        rows_sorted = rows\n",
    "\n",
    "    return rows_sorted[:top]\n",
    "\n",
    "def unique_to_site(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 50\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Words present in site1 but NOT in site2 (ordered by frequency in site1).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency\n",
    "        FROM {table} a\n",
    "        LEFT JOIN {table} b ON a.word = b.word AND b.site = ?\n",
    "        WHERE a.site = ? AND b.word IS NULL\n",
    "        ORDER BY a.frequency DESC\n",
    "        LIMIT ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site2, site1, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "# -------------------------\n",
    "# Example end-to-end usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: single site (IISER Mohali)\n",
    "    mohali_url = \"https://www.iisermohali.ac.in/\"\n",
    "    print(\"Scraping:\", mohali_url)\n",
    "    cleaned_mohali_text = scrape_text_from_url(mohali_url)\n",
    "    word_counts_mohali = count_words(cleaned_mohali_text)\n",
    "    print(\"Unique words (Mohali):\", len(word_counts_mohali))\n",
    "    print(\"Top 10 (Mohali):\", word_counts_mohali.most_common(10))\n",
    "\n",
    "    # Store in DB (accumulate=True => add counts if site already present)\n",
    "    store_site_word_counts(word_counts_mohali, site=mohali_url, accumulate=True)\n",
    "    print(\"Stored Mohali counts in DB.\")\n",
    "\n",
    "    # Example 2: another site (IISER Pune) - optional\n",
    "    pune_url = \"https://www.iiserpune.ac.in/\"\n",
    "    print(\"\\nScraping:\", pune_url)\n",
    "    cleaned_pune_text = scrape_text_from_url(pune_url)\n",
    "    word_counts_pune = count_words(cleaned_pune_text)\n",
    "    store_site_word_counts(word_counts_pune, site=pune_url, accumulate=True)\n",
    "    print(\"Stored Pune counts in DB.\")\n",
    "\n",
    "    # Read and display top words for each site\n",
    "    print(\"\\nTop words - Mohali:\")\n",
    "    for w, f in read_top_by_site(mohali_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "    print(\"\\nTop words - Pune:\")\n",
    "    for w, f in read_top_by_site(pune_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "\n",
    "    # Compare common words\n",
    "    print(\"\\nTop common words (by sum of frequencies):\")\n",
    "    for word, f1, f2 in compare_sites_common(mohali_url, pune_url, top=25, sort_by=\"sum\"):\n",
    "        print(f\"{word:>15}  Mohali:{f1:5d}  Pune:{f2:5d}\")\n",
    "\n",
    "    # Words unique to Mohali (not in Pune)\n",
    "    print(\"\\nTop words unique to Mohali (not in Pune):\")\n",
    "    for w, f in unique_to_site(mohali_url, pune_url, top=30):\n",
    "        print(f\"{w:>15}  {f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
