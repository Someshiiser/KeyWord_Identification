{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811292fd",
   "metadata": {},
   "source": [
    "somesh code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc374b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.iisermohali.ac.in/\n",
      "Unique words (Mohali): 212\n",
      "Top 10 (Mohali): [('iiser', 15), ('mohali', 14), ('of', 8), ('science', 6), ('sciences', 6), ('committee', 6), ('research', 5), ('day', 5), ('students', 5), ('institute', 4)]\n",
      "Stored Mohali counts in DB.\n",
      "\n",
      "Scraping: https://www.iiserpune.ac.in/\n",
      "Stored Pune counts in DB.\n",
      "\n",
      "Top words - Mohali:\n",
      "          iiser  30\n",
      "         mohali  28\n",
      "             of  16\n",
      "      committee  12\n",
      "        science  12\n",
      "       sciences  12\n",
      "            day  10\n",
      "       research  10\n",
      "       students  10\n",
      "             at  8\n",
      "        faculty  8\n",
      "      institute  8\n",
      "           news  8\n",
      "      academics  6\n",
      "            ble  6\n",
      "             by  6\n",
      "       calendar  6\n",
      "    celebration  6\n",
      "     committees  6\n",
      "         events  6\n",
      "\n",
      "Top words - Pune:\n",
      "            and  72\n",
      "            the  44\n",
      "             in  40\n",
      "             of  34\n",
      "          iiser  30\n",
      "           pune  30\n",
      "       research  30\n",
      "         events  22\n",
      "             on  22\n",
      "         campus  20\n",
      "        faculty  20\n",
      "             at  18\n",
      "            for  18\n",
      "        science  18\n",
      "             by  16\n",
      "        fellows  16\n",
      "        members  16\n",
      "           more  16\n",
      "           news  16\n",
      "         posted  16\n",
      "\n",
      "Top common words (by sum of frequencies):\n",
      "          iiser  Mohali:   30  Pune:   30\n",
      "             of  Mohali:   16  Pune:   34\n",
      "            the  Mohali:    6  Pune:   44\n",
      "             in  Mohali:    4  Pune:   40\n",
      "       research  Mohali:   10  Pune:   30\n",
      "        science  Mohali:   12  Pune:   18\n",
      "         events  Mohali:    6  Pune:   22\n",
      "        faculty  Mohali:    8  Pune:   20\n",
      "             at  Mohali:    8  Pune:   18\n",
      "         campus  Mohali:    4  Pune:   20\n",
      "           news  Mohali:    8  Pune:   16\n",
      "             by  Mohali:    6  Pune:   16\n",
      "            for  Mohali:    4  Pune:   18\n",
      "       sciences  Mohali:   12  Pune:    8\n",
      "             dr  Mohali:    4  Pune:   14\n",
      "        members  Mohali:    2  Pune:   16\n",
      "             to  Mohali:    4  Pune:   14\n",
      "         alumni  Mohali:    2  Pune:   14\n",
      "            day  Mohali:   10  Pune:    6\n",
      "             as  Mohali:    2  Pune:   12\n",
      "      committee  Mohali:   12  Pune:    2\n",
      "        elected  Mohali:    2  Pune:   12\n",
      "     facilities  Mohali:    6  Pune:    8\n",
      "          india  Mohali:    4  Pune:   10\n",
      "          media  Mohali:    2  Pune:   12\n",
      "\n",
      "Top words unique to Mohali (not in Pune):\n",
      "         mohali  28\n",
      "    celebration  6\n",
      "     committees  6\n",
      "      president  6\n",
      "           vice  6\n",
      "   achievements  4\n",
      "        awarded  4\n",
      "        careers  4\n",
      "     complaints  4\n",
      "       computer  4\n",
      "       festival  4\n",
      "        holiday  4\n",
      "            icc  4\n",
      "     institutes  4\n",
      "          rules  4\n",
      "         school  4\n",
      "         system  4\n",
      "             th  4\n",
      "        website  4\n",
      "      academies  2\n",
      "         across  2\n",
      " administration  2\n",
      " administrative  2\n",
      "      admission  2\n",
      " advertisements  2\n",
      "       advisory  2\n",
      "       agencies  2\n",
      "           anil  2\n",
      "          anjan  2\n",
      "         annual  2\n"
     ]
    }
   ],
   "source": [
    "# Complete end-to-end: scrape -> clean -> count -> store -> compare\n",
    "# Requirements: requests, beautifulsoup4\n",
    "# Run: pip install requests beautifulsoup4  (if not installed)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -------------------------\n",
    "# Scrape + clean functions\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove non-letters except spaces,\n",
    "    remove stray single letters except 'a' and 'i', normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)               # keep only a-z and whitespace\n",
    "    text = re.sub(r'\\b(?![ai]\\b)[a-z]\\b', ' ', text)    # remove single letters except 'a' and 'i'\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()            # collapse spaces\n",
    "    return text\n",
    "\n",
    "def scrape_text_from_url(url: str, valid_tags=None, timeout=12) -> str:\n",
    "    \"\"\"\n",
    "    Fetch URL, parse HTML, extract text from valid_tags, then clean it.\n",
    "    Returns cleaned text string.\n",
    "    \"\"\"\n",
    "    if valid_tags is None:\n",
    "        valid_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']\n",
    "\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch {url} (status {response.status_code})\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pieces = [el.get_text(separator=' ', strip=True) for el in soup.find_all(valid_tags)]\n",
    "    raw_text = \" \".join(pieces)\n",
    "    return clean_text(raw_text)\n",
    "\n",
    "# -------------------------\n",
    "# Counting function\n",
    "# -------------------------\n",
    "def count_words(cleaned_text: str) -> Counter:\n",
    "    words = cleaned_text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "# -------------------------\n",
    "# SQLite storage functions (single table for all sites)\n",
    "# -------------------------\n",
    "def store_site_word_counts(\n",
    "    word_counts: Counter,\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    accumulate: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Store counts for a given site into SQLite.\n",
    "    - accumulate=True : add new counts to existing (useful for incremental scraping)\n",
    "    - accumulate=False: replace stored counts for that site (snapshot)\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table holding rows for many sites (site, word, frequency)\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            site TEXT NOT NULL,\n",
    "            word TEXT NOT NULL,\n",
    "            frequency INTEGER NOT NULL,\n",
    "            UNIQUE(site, word)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    if accumulate:\n",
    "        # add new counts to existing frequency\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = {table}.frequency + excluded.frequency\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # replace stored frequency with the new one\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = excluded.frequency\n",
    "        \"\"\"\n",
    "\n",
    "    for w, freq in word_counts.items():\n",
    "        cur.execute(upsert_sql, (site, w, int(freq)))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def read_top_by_site(\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 20\n",
    ") -> List[Tuple[str, int]]:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT word, frequency FROM {table} WHERE site = ? ORDER BY frequency DESC LIMIT ?\", (site, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "def compare_sites_common(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 30,\n",
    "    sort_by: str = \"sum\"  # \"sum\" | \"min\" | \"diff\"\n",
    ") -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Return common words between site1 and site2 as (word, freq_site1, freq_site2).\n",
    "    sort_by:\n",
    "      - 'sum' : sort by freq1+freq2 descending (default)\n",
    "      - 'min' : sort by min(freq1,freq2) descending -> words both strongly used\n",
    "      - 'diff': sort by absolute difference descending -> most differently-used words\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency AS f1, b.frequency AS f2\n",
    "        FROM {table} a\n",
    "        JOIN {table} b ON a.word = b.word\n",
    "        WHERE a.site = ? AND b.site = ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site1, site2))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if sort_by == \"sum\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: (r[1] + r[2]), reverse=True)\n",
    "    elif sort_by == \"min\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: min(r[1], r[2]), reverse=True)\n",
    "    elif sort_by == \"diff\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: abs(r[1] - r[2]), reverse=True)\n",
    "    else:\n",
    "        rows_sorted = rows\n",
    "\n",
    "    return rows_sorted[:top]\n",
    "\n",
    "def unique_to_site(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 50\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Words present in site1 but NOT in site2 (ordered by frequency in site1).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency\n",
    "        FROM {table} a\n",
    "        LEFT JOIN {table} b ON a.word = b.word AND b.site = ?\n",
    "        WHERE a.site = ? AND b.word IS NULL\n",
    "        ORDER BY a.frequency DESC\n",
    "        LIMIT ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site2, site1, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "# -------------------------\n",
    "# Example end-to-end usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: single site (IISER Mohali)\n",
    "    mohali_url = \"https://www.iisermohali.ac.in/\"\n",
    "    print(\"Scraping:\", mohali_url)\n",
    "    cleaned_mohali_text = scrape_text_from_url(mohali_url)\n",
    "    word_counts_mohali = count_words(cleaned_mohali_text)\n",
    "    print(\"Unique words (Mohali):\", len(word_counts_mohali))\n",
    "    print(\"Top 10 (Mohali):\", word_counts_mohali.most_common(10))\n",
    "\n",
    "    # Store in DB (accumulate=True => add counts if site already present)\n",
    "    store_site_word_counts(word_counts_mohali, site=mohali_url, accumulate=True)\n",
    "    print(\"Stored Mohali counts in DB.\")\n",
    "\n",
    "    # Example 2: another site (IISER Pune) - optional\n",
    "    pune_url = \"https://www.iiserpune.ac.in/\"\n",
    "    print(\"\\nScraping:\", pune_url)\n",
    "    cleaned_pune_text = scrape_text_from_url(pune_url)\n",
    "    word_counts_pune = count_words(cleaned_pune_text)\n",
    "    store_site_word_counts(word_counts_pune, site=pune_url, accumulate=True)\n",
    "    print(\"Stored Pune counts in DB.\")\n",
    "\n",
    "    # Read and display top words for each site\n",
    "    print(\"\\nTop words - Mohali:\")\n",
    "    for w, f in read_top_by_site(mohali_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "    print(\"\\nTop words - Pune:\")\n",
    "    for w, f in read_top_by_site(pune_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "\n",
    "    # Compare common words\n",
    "    print(\"\\nTop common words (by sum of frequencies):\")\n",
    "    for word, f1, f2 in compare_sites_common(mohali_url, pune_url, top=25, sort_by=\"sum\"):\n",
    "        print(f\"{word:>15}  Mohali:{f1:5d}  Pune:{f2:5d}\")\n",
    "\n",
    "    # Words unique to Mohali (not in Pune)\n",
    "    print(\"\\nTop words unique to Mohali (not in Pune):\")\n",
    "    for w, f in unique_to_site(mohali_url, pune_url, top=30):\n",
    "        print(f\"{w:>15}  {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68726e56",
   "metadata": {},
   "source": [
    "mine(with chat gpt help ofcourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bec134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing https://www.iisermohali.ac.in/ ---\n",
      "Scraped 344 words from https://www.iisermohali.ac.in/\n",
      "Top 10 Keywords:\n",
      "  iiser                           score=0.004085\n",
      "  mohali                          score=0.004981\n",
      "  day                             score=0.016648\n",
      "  research                        score=0.016648\n",
      "  committee                       score=0.017783\n",
      "  students                        score=0.018807\n",
      "  science                         score=0.019928\n",
      "  sciences                        score=0.019928\n",
      "  celebration                     score=0.020317\n",
      "  faculty                         score=0.021518\n",
      "  ble                             score=0.022610\n",
      "  president                       score=0.022610\n",
      "  vice                            score=0.022610\n",
      "  institute                       score=0.023509\n",
      "  hon                             score=0.024999\n",
      "  academics                       score=0.026343\n",
      "  calendar                        score=0.027477\n",
      "  facilities                      score=0.027477\n",
      "  events                          score=0.030039\n",
      "  policy                          score=0.030039\n",
      "\n",
      "--- Processing https://www.iiserpune.ac.in/ ---\n",
      "Scraped 730 words from https://www.iiserpune.ac.in/\n",
      "Top 10 Keywords:\n",
      "  iiser                           score=0.002319\n",
      "  pune                            score=0.002666\n",
      "  research                        score=0.003853\n",
      "  faculty                         score=0.004192\n",
      "  members                         score=0.004882\n",
      "  campus                          score=0.006308\n",
      "  events                          score=0.006472\n",
      "  posted                          score=0.006657\n",
      "  science                         score=0.006810\n",
      "  fellows                         score=0.006975\n",
      "  elected                         score=0.007267\n",
      "  read                            score=0.007848\n",
      "  alumni                          score=0.008624\n",
      "  media                           score=0.009034\n",
      "  national                        score=0.009977\n",
      "  prof                            score=0.010958\n",
      "  india                           score=0.012528\n",
      "  programme                       score=0.012745\n",
      "  sciences                        score=0.015323\n",
      "  book                            score=0.016534\n",
      "\n",
      "✅ YAKE keyword extraction completed and stored in database.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full Program: Web Scraper + Text Cleaner + YAKE Keyword Extraction + SQLite Storage\n",
    "-----------------------------------------------------------------------------------\n",
    "This program:\n",
    "1. Scrapes text from a given website (using requests + BeautifulSoup).\n",
    "2. Cleans the text (removes noise, normalizes case and spacing).\n",
    "3. Extracts important keywords/phrases using YAKE.\n",
    "4. Stores results into a SQLite database.\n",
    "5. Allows comparison between sites by keyword importance.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "import yake\n",
    "from typing import List, Tuple\n",
    "\n",
    "# =========================================================\n",
    "# STEP 1: Text cleaning\n",
    "# =========================================================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove non-alphabetic characters (except spaces),\n",
    "    and normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()                          # Normalize case\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)        # Keep only letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # Collapse multiple spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 2: Scraping text from a webpage\n",
    "# =========================================================\n",
    "def scrape_text_from_url(url: str, valid_tags=None, timeout: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Fetch webpage content, extract readable text, and clean it.\n",
    "    \"\"\"\n",
    "    if valid_tags is None:\n",
    "        valid_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'li']\n",
    "\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch {url} (status {response.status_code})\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract text only from specific HTML tags\n",
    "    pieces = [el.get_text(separator=' ', strip=True) for el in soup.find_all(valid_tags)]\n",
    "    raw_text = \" \".join(pieces)\n",
    "\n",
    "    # Clean and return\n",
    "    return clean_text(raw_text)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 3: YAKE Keyword Extraction\n",
    "# =========================================================\n",
    "def extract_keywords_yake(text: str, max_keywords: int = 20) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract top keywords using YAKE.\n",
    "    Lower score => more relevant keyword.\n",
    "    \"\"\"\n",
    "    # Configure YAKE parameters\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan=\"en\",              # language\n",
    "        n=1,                   # max n-gram size (1 = single words, 3 = up to trigrams)\n",
    "        dedupLim=0.9,          # threshold for merging similar words\n",
    "        top=max_keywords,      # number of keywords to extract\n",
    "        features=None          # use default YAKE features\n",
    "    )\n",
    "\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return keywords  # Returns list of (keyword, score)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 4: SQLite Storage\n",
    "# =========================================================\n",
    "def store_keywords(\n",
    "    keywords: List[Tuple[str, float]],\n",
    "    site: str,\n",
    "    db_path: str = \"keywords.db\",\n",
    "    table: str = \"keyword_scores\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Store YAKE keywords and scores for a site into SQLite database.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            site TEXT NOT NULL,\n",
    "            keyword TEXT NOT NULL,\n",
    "            score REAL NOT NULL,\n",
    "            UNIQUE(site, keyword)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert or update existing entries\n",
    "    for kw, score in keywords:\n",
    "        cur.execute(f\"\"\"\n",
    "            INSERT INTO {table} (site, keyword, score)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, keyword)\n",
    "            DO UPDATE SET score = excluded.score\n",
    "        \"\"\", (site, kw, score))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 5: Read keywords from DB\n",
    "# =========================================================\n",
    "def read_top_keywords(site: str, db_path: str = \"keywords.db\", table: str = \"keyword_scores\", top: int = 10):\n",
    "    \"\"\"\n",
    "    Fetch top N keywords (lowest YAKE score = most important).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT keyword, score FROM {table}\n",
    "        WHERE site = ?\n",
    "        ORDER BY score ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (site, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 6: Example run (compare two IISER sites)\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example sites\n",
    "    mohali_url = \"https://www.iisermohali.ac.in/\"\n",
    "    pune_url   = \"https://www.iiserpune.ac.in/\"\n",
    "\n",
    "    for site in [mohali_url, pune_url]:\n",
    "        print(f\"\\n--- Processing {site} ---\")\n",
    "\n",
    "        # Step 1: Scrape + Clean\n",
    "        text = scrape_text_from_url(site)\n",
    "        print(f\"Scraped {len(text.split())} words from {site}\")\n",
    "\n",
    "        # Step 2: Extract keywords using YAKE\n",
    "        keywords = extract_keywords_yake(text, max_keywords=20)\n",
    "\n",
    "        # Step 3: Store into SQLite\n",
    "        store_keywords(keywords, site)\n",
    "\n",
    "        # Step 4: Show top keywords\n",
    "        print(\"Top 10 Keywords:\")\n",
    "        for kw, score in read_top_keywords(site, top=30):\n",
    "            print(f\"  {kw:30}  score={score:.6f}\")\n",
    "\n",
    "    print(\"\\n✅ YAKE keyword extraction completed and stored in database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsenv2)",
   "language": "python",
   "name": "dsenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
