{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811292fd",
   "metadata": {},
   "source": [
    "somesh code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc374b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Complete end-to-end: scrape -> clean -> count -> store -> compare\n",
    "# Requirements: requests, beautifulsoup4\n",
    "# Run: pip install requests beautifulsoup4  (if not installed)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -------------------------\n",
    "# Scrape + clean functions\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove non-letters except spaces,\n",
    "    remove stray single letters except 'a' and 'i', normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)               # keep only a-z and whitespace\n",
    "    text = re.sub(r'\\b(?![ai]\\b)[a-z]\\b', ' ', text)    # remove single letters except 'a' and 'i'\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()            # collapse spaces\n",
    "    return text\n",
    "\n",
    "def scrape_text_from_url(url: str, valid_tags=None, timeout=12) -> str:\n",
    "    \"\"\"\n",
    "    Fetch URL, parse HTML, extract text from valid_tags, then clean it.\n",
    "    Returns cleaned text string.\n",
    "    \"\"\"\n",
    "    if valid_tags is None:\n",
    "        valid_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']\n",
    "\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch {url} (status {response.status_code})\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pieces = [el.get_text(separator=' ', strip=True) for el in soup.find_all(valid_tags)]\n",
    "    raw_text = \" \".join(pieces)\n",
    "    return clean_text(raw_text)\n",
    "\n",
    "# -------------------------\n",
    "# Counting function\n",
    "# -------------------------\n",
    "def count_words(cleaned_text: str) -> Counter:\n",
    "    words = cleaned_text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "# -------------------------\n",
    "# SQLite storage functions (single table for all sites)\n",
    "# -------------------------\n",
    "def store_site_word_counts(\n",
    "    word_counts: Counter,\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    accumulate: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Store counts for a given site into SQLite.\n",
    "    - accumulate=True : add new counts to existing (useful for incremental scraping)\n",
    "    - accumulate=False: replace stored counts for that site (snapshot)\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table holding rows for many sites (site, word, frequency)\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            site TEXT NOT NULL,\n",
    "            word TEXT NOT NULL,\n",
    "            frequency INTEGER NOT NULL,\n",
    "            UNIQUE(site, word)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    if accumulate:\n",
    "        # add new counts to existing frequency\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = {table}.frequency + excluded.frequency\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # replace stored frequency with the new one\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO {table} (site, word, frequency)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, word) DO UPDATE\n",
    "              SET frequency = excluded.frequency\n",
    "        \"\"\"\n",
    "\n",
    "    for w, freq in word_counts.items():\n",
    "        cur.execute(upsert_sql, (site, w, int(freq)))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def read_top_by_site(\n",
    "    site: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 20\n",
    ") -> List[Tuple[str, int]]:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT word, frequency FROM {table} WHERE site = ? ORDER BY frequency DESC LIMIT ?\", (site, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "def compare_sites_common(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 30,\n",
    "    sort_by: str = \"sum\"  # \"sum\" | \"min\" | \"diff\"\n",
    ") -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Return common words between site1 and site2 as (word, freq_site1, freq_site2).\n",
    "    sort_by:\n",
    "      - 'sum' : sort by freq1+freq2 descending (default)\n",
    "      - 'min' : sort by min(freq1,freq2) descending -> words both strongly used\n",
    "      - 'diff': sort by absolute difference descending -> most differently-used words\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency AS f1, b.frequency AS f2\n",
    "        FROM {table} a\n",
    "        JOIN {table} b ON a.word = b.word\n",
    "        WHERE a.site = ? AND b.site = ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site1, site2))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if sort_by == \"sum\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: (r[1] + r[2]), reverse=True)\n",
    "    elif sort_by == \"min\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: min(r[1], r[2]), reverse=True)\n",
    "    elif sort_by == \"diff\":\n",
    "        rows_sorted = sorted(rows, key=lambda r: abs(r[1] - r[2]), reverse=True)\n",
    "    else:\n",
    "        rows_sorted = rows\n",
    "\n",
    "    return rows_sorted[:top]\n",
    "\n",
    "def unique_to_site(\n",
    "    site1: str,\n",
    "    site2: str,\n",
    "    db_path: str = \"words.db\",\n",
    "    table: str = \"word_frequency\",\n",
    "    top: int = 50\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Words present in site1 but NOT in site2 (ordered by frequency in site1).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    sql = f\"\"\"\n",
    "        SELECT a.word, a.frequency\n",
    "        FROM {table} a\n",
    "        LEFT JOIN {table} b ON a.word = b.word AND b.site = ?\n",
    "        WHERE a.site = ? AND b.word IS NULL\n",
    "        ORDER BY a.frequency DESC\n",
    "        LIMIT ?\n",
    "    \"\"\"\n",
    "    cur.execute(sql, (site2, site1, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "# -------------------------\n",
    "# Example end-to-end usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: single site (IISER Mohali)\n",
    "    mohali_url = \"https://www.iisermohali.ac.in/\"\n",
    "    print(\"Scraping:\", mohali_url)\n",
    "    cleaned_mohali_text = scrape_text_from_url(mohali_url)\n",
    "    word_counts_mohali = count_words(cleaned_mohali_text)\n",
    "    print(\"Unique words (Mohali):\", len(word_counts_mohali))\n",
    "    print(\"Top 10 (Mohali):\", word_counts_mohali.most_common(10))\n",
    "\n",
    "    # Store in DB (accumulate=True => add counts if site already present)\n",
    "    store_site_word_counts(word_counts_mohali, site=mohali_url, accumulate=True)\n",
    "    print(\"Stored Mohali counts in DB.\")\n",
    "\n",
    "    # Example 2: another site (IISER Pune) - optional\n",
    "    pune_url = \"https://www.iiserpune.ac.in/\"\n",
    "    print(\"\\nScraping:\", pune_url)\n",
    "    cleaned_pune_text = scrape_text_from_url(pune_url)\n",
    "    word_counts_pune = count_words(cleaned_pune_text)\n",
    "    store_site_word_counts(word_counts_pune, site=pune_url, accumulate=True)\n",
    "    print(\"Stored Pune counts in DB.\")\n",
    "\n",
    "    # Read and display top words for each site\n",
    "    print(\"\\nTop words - Mohali:\")\n",
    "    for w, f in read_top_by_site(mohali_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "    print(\"\\nTop words - Pune:\")\n",
    "    for w, f in read_top_by_site(pune_url, top=20):\n",
    "        print(f\"{w:>15}  {f}\")\n",
    "\n",
    "    # Compare common words\n",
    "    print(\"\\nTop common words (by sum of frequencies):\")\n",
    "    for word, f1, f2 in compare_sites_common(mohali_url, pune_url, top=25, sort_by=\"sum\"):\n",
    "        print(f\"{word:>15}  Mohali:{f1:5d}  Pune:{f2:5d}\")\n",
    "\n",
    "    # Words unique to Mohali (not in Pune)\n",
    "    print(\"\\nTop words unique to Mohali (not in Pune):\")\n",
    "    for w, f in unique_to_site(mohali_url, pune_url, top=30):\n",
    "        print(f\"{w:>15}  {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68726e56",
   "metadata": {},
   "source": [
    "mine(with chat gpt help ofcourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec134e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full Program: Web Scraper + Text Cleaner + YAKE Keyword Extraction + SQLite Storage\n",
    "-----------------------------------------------------------------------------------\n",
    "This program:\n",
    "1. Scrapes text from a given website (using requests + BeautifulSoup).\n",
    "2. Cleans the text (removes noise, normalizes case and spacing).\n",
    "3. Extracts important keywords/phrases using YAKE.\n",
    "4. Stores results into a SQLite database.\n",
    "5. Allows comparison between sites by keyword importance.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "import yake\n",
    "from typing import List, Tuple\n",
    "\n",
    "# =========================================================\n",
    "# STEP 1: Text cleaning\n",
    "# =========================================================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove non-alphabetic characters (except spaces),\n",
    "    and normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()                          # Normalize case\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)        # Keep only letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # Collapse multiple spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 2: Scraping text from a webpage\n",
    "# =========================================================\n",
    "def scrape_text_from_url(url: str, valid_tags=None, timeout: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Fetch webpage content, extract readable text, and clean it.\n",
    "    \"\"\"\n",
    "    if valid_tags is None:\n",
    "        valid_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'li']\n",
    "\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch {url} (status {response.status_code})\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract text only from specific HTML tags\n",
    "    pieces = [el.get_text(separator=' ', strip=True) for el in soup.find_all(valid_tags)]\n",
    "    raw_text = \" \".join(pieces)\n",
    "\n",
    "    # Clean and return\n",
    "    return clean_text(raw_text)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 3: YAKE Keyword Extraction\n",
    "# =========================================================\n",
    "def extract_keywords_yake(text: str, max_keywords: int = 20) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract top keywords using YAKE.\n",
    "    Lower score => more relevant keyword.\n",
    "    \"\"\"\n",
    "    # Configure YAKE parameters\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan=\"en\",              # language\n",
    "        n=1,                   # max n-gram size (1 = single words, 3 = up to trigrams)\n",
    "        dedupLim=0.9,          # threshold for merging similar words\n",
    "        top=max_keywords,      # number of keywords to extract\n",
    "        features=None          # use default YAKE features\n",
    "    )\n",
    "\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return keywords  # Returns list of (keyword, score)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 4: SQLite Storage\n",
    "# =========================================================\n",
    "def store_keywords(\n",
    "    keywords: List[Tuple[str, float]],\n",
    "    site: str,\n",
    "    db_path: str = \"keywords.db\",\n",
    "    table: str = \"keyword_scores\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Store YAKE keywords and scores for a site into SQLite database.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            site TEXT NOT NULL,\n",
    "            keyword TEXT NOT NULL,\n",
    "            score REAL NOT NULL,\n",
    "            UNIQUE(site, keyword)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert or update existing entries\n",
    "    for kw, score in keywords:\n",
    "        cur.execute(f\"\"\"\n",
    "            INSERT INTO {table} (site, keyword, score)\n",
    "            VALUES (?, ?, ?)\n",
    "            ON CONFLICT(site, keyword)\n",
    "            DO UPDATE SET score = excluded.score\n",
    "        \"\"\", (site, kw, score))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 5: Read keywords from DB\n",
    "# =========================================================\n",
    "def read_top_keywords(site: str, db_path: str = \"keywords.db\", table: str = \"keyword_scores\", top: int = 10):\n",
    "    \"\"\"\n",
    "    Fetch top N keywords (lowest YAKE score = most important).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT keyword, score FROM {table}\n",
    "        WHERE site = ?\n",
    "        ORDER BY score ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (site, top))\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# STEP 6: Example run (compare two IISER sites)\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example sites\n",
    "    mohali_url = \"https://www.iisermohali.ac.in/\"\n",
    "    pune_url   = \"https://www.iiserpune.ac.in/\"\n",
    "\n",
    "    for site in [mohali_url, pune_url]:\n",
    "        print(f\"\\n--- Processing {site} ---\")\n",
    "\n",
    "        # Step 1: Scrape + Clean\n",
    "        text = scrape_text_from_url(site)\n",
    "        print(f\"Scraped {len(text.split())} words from {site}\")\n",
    "\n",
    "        # Step 2: Extract keywords using YAKE\n",
    "        keywords = extract_keywords_yake(text, max_keywords=20)\n",
    "\n",
    "        # Step 3: Store into SQLite\n",
    "        store_keywords(keywords, site)\n",
    "\n",
    "        # Step 4: Show top keywords\n",
    "        print(\"Top 10 Keywords:\")\n",
    "        for kw, score in read_top_keywords(site, top=10):\n",
    "            print(f\"  {kw:30}  score={score:.6f}\")\n",
    "\n",
    "    print(\"\\n✅ YAKE keyword extraction completed and stored in database.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
